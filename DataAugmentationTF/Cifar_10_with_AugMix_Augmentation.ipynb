{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar-10  with AugMix Augmentation",
      "provenance": [],
      "authorship_tag": "ABX9TyN4sUd2yH+g3V5RL5UR1Hks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayulockin/DataAugmentationTF/blob/master/Cifar_10_with_AugMix_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z2EHAFizLNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "bc8c31dd-15e6-401a-ef57-4e274c457d25"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "路路路路路路路路路路\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5sM3xsQO7tK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "8cb58c8e-cab0-4617-f73d-6ece8a225551"
      },
      "source": [
        "!git clone https://github.com/ayulockin/AugMix_TF2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AugMix_TF2'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/139)\u001b[K\rremote: Counting objects:   1% (2/139)\u001b[K\rremote: Counting objects:   2% (3/139)\u001b[K\rremote: Counting objects:   3% (5/139)\u001b[K\rremote: Counting objects:   4% (6/139)\u001b[K\rremote: Counting objects:   5% (7/139)\u001b[K\rremote: Counting objects:   6% (9/139)\u001b[K\rremote: Counting objects:   7% (10/139)\u001b[K\rremote: Counting objects:   8% (12/139)\u001b[K\rremote: Counting objects:   9% (13/139)\u001b[K\rremote: Counting objects:  10% (14/139)\u001b[K\rremote: Counting objects:  11% (16/139)\u001b[K\rremote: Counting objects:  12% (17/139)\u001b[K\rremote: Counting objects:  13% (19/139)\u001b[K\rremote: Counting objects:  14% (20/139)\u001b[K\rremote: Counting objects:  15% (21/139)\u001b[K\rremote: Counting objects:  16% (23/139)\u001b[K\rremote: Counting objects:  17% (24/139)\u001b[K\rremote: Counting objects:  18% (26/139)\u001b[K\rremote: Counting objects:  19% (27/139)\u001b[K\rremote: Counting objects:  20% (28/139)\u001b[K\rremote: Counting objects:  21% (30/139)\u001b[K\rremote: Counting objects:  22% (31/139)\u001b[K\rremote: Counting objects:  23% (32/139)\u001b[K\rremote: Counting objects:  24% (34/139)\u001b[K\rremote: Counting objects:  25% (35/139)\u001b[K\rremote: Counting objects:  26% (37/139)\u001b[K\rremote: Counting objects:  27% (38/139)\u001b[K\rremote: Counting objects:  28% (39/139)\u001b[K\rremote: Counting objects:  29% (41/139)\u001b[K\rremote: Counting objects:  30% (42/139)\u001b[K\rremote: Counting objects:  31% (44/139)\u001b[K\rremote: Counting objects:  32% (45/139)\u001b[K\rremote: Counting objects:  33% (46/139)\u001b[K\rremote: Counting objects:  34% (48/139)\u001b[K\rremote: Counting objects:  35% (49/139)\u001b[K\rremote: Counting objects:  36% (51/139)\u001b[K\rremote: Counting objects:  37% (52/139)\u001b[K\rremote: Counting objects:  38% (53/139)\u001b[K\rremote: Counting objects:  39% (55/139)\u001b[K\rremote: Counting objects:  40% (56/139)\u001b[K\rremote: Counting objects:  41% (57/139)\u001b[K\rremote: Counting objects:  42% (59/139)\u001b[K\rremote: Counting objects:  43% (60/139)\u001b[K\rremote: Counting objects:  44% (62/139)\u001b[K\rremote: Counting objects:  45% (63/139)\u001b[K\rremote: Counting objects:  46% (64/139)\u001b[K\rremote: Counting objects:  47% (66/139)\u001b[K\rremote: Counting objects:  48% (67/139)\u001b[K\rremote: Counting objects:  49% (69/139)\u001b[K\rremote: Counting objects:  50% (70/139)\u001b[K\rremote: Counting objects:  51% (71/139)\u001b[K\rremote: Counting objects:  52% (73/139)\u001b[K\rremote: Counting objects:  53% (74/139)\u001b[K\rremote: Counting objects:  54% (76/139)\u001b[K\rremote: Counting objects:  55% (77/139)\u001b[K\rremote: Counting objects:  56% (78/139)\u001b[K\rremote: Counting objects:  57% (80/139)\u001b[K\rremote: Counting objects:  58% (81/139)\u001b[K\rremote: Counting objects:  59% (83/139)\u001b[K\rremote: Counting objects:  60% (84/139)\u001b[K\rremote: Counting objects:  61% (85/139)\u001b[K\rremote: Counting objects:  62% (87/139)\u001b[K\rremote: Counting objects:  63% (88/139)\u001b[K\rremote: Counting objects:  64% (89/139)\u001b[K\rremote: Counting objects:  65% (91/139)\u001b[K\rremote: Counting objects:  66% (92/139)\u001b[K\rremote: Counting objects:  67% (94/139)\u001b[K\rremote: Counting objects:  68% (95/139)\u001b[K\rremote: Counting objects:  69% (96/139)\u001b[K\rremote: Counting objects:  70% (98/139)\u001b[K\rremote: Counting objects:  71% (99/139)\u001b[K\rremote: Counting objects:  72% (101/139)\u001b[K\rremote: Counting objects:  73% (102/139)\u001b[K\rremote: Counting objects:  74% (103/139)\u001b[K\rremote: Counting objects:  75% (105/139)\u001b[K\rremote: Counting objects:  76% (106/139)\u001b[K\rremote: Counting objects:  77% (108/139)\u001b[K\rremote: Counting objects:  78% (109/139)\u001b[K\rremote: Counting objects:  79% (110/139)\u001b[K\rremote: Counting objects:  80% (112/139)\u001b[K\rremote: Counting objects:  81% (113/139)\u001b[K\rremote: Counting objects:  82% (114/139)\u001b[K\rremote: Counting objects:  83% (116/139)\u001b[K\rremote: Counting objects:  84% (117/139)\u001b[K\rremote: Counting objects:  85% (119/139)\u001b[K\rremote: Counting objects:  86% (120/139)\u001b[K\rremote: Counting objects:  87% (121/139)\u001b[K\rremote: Counting objects:  88% (123/139)\u001b[K\rremote: Counting objects:  89% (124/139)\u001b[K\rremote: Counting objects:  90% (126/139)\u001b[K\rremote: Counting objects:  91% (127/139)\u001b[K\rremote: Counting objects:  92% (128/139)\u001b[K\rremote: Counting objects:  93% (130/139)\u001b[K\rremote: Counting objects:  94% (131/139)\u001b[K\rremote: Counting objects:  95% (133/139)\u001b[K\rremote: Counting objects:  96% (134/139)\u001b[K\rremote: Counting objects:  97% (135/139)\u001b[K\rremote: Counting objects:  98% (137/139)\u001b[K\rremote: Counting objects:  99% (138/139)\u001b[K\rremote: Counting objects: 100% (139/139)\u001b[K\rremote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/103)\u001b[K\rremote: Compressing objects:   1% (2/103)\u001b[K\rremote: Compressing objects:   2% (3/103)\u001b[K\rremote: Compressing objects:   3% (4/103)\u001b[K\rremote: Compressing objects:   4% (5/103)\u001b[K\rremote: Compressing objects:   5% (6/103)\u001b[K\rremote: Compressing objects:   6% (7/103)\u001b[K\rremote: Compressing objects:   7% (8/103)\u001b[K\rremote: Compressing objects:   8% (9/103)\u001b[K\rremote: Compressing objects:   9% (10/103)\u001b[K\rremote: Compressing objects:  10% (11/103)\u001b[K\rremote: Compressing objects:  11% (12/103)\u001b[K\rremote: Compressing objects:  12% (13/103)\u001b[K\rremote: Compressing objects:  13% (14/103)\u001b[K\rremote: Compressing objects:  14% (15/103)\u001b[K\rremote: Compressing objects:  15% (16/103)\u001b[K\rremote: Compressing objects:  16% (17/103)\u001b[K\rremote: Compressing objects:  17% (18/103)\u001b[K\rremote: Compressing objects:  18% (19/103)\u001b[K\rremote: Compressing objects:  19% (20/103)\u001b[K\rremote: Compressing objects:  20% (21/103)\u001b[K\rremote: Compressing objects:  21% (22/103)\u001b[K\rremote: Compressing objects:  22% (23/103)\u001b[K\rremote: Compressing objects:  23% (24/103)\u001b[K\rremote: Compressing objects:  24% (25/103)\u001b[K\rremote: Compressing objects:  25% (26/103)\u001b[K\rremote: Compressing objects:  26% (27/103)\u001b[K\rremote: Compressing objects:  27% (28/103)\u001b[K\rremote: Compressing objects:  28% (29/103)\u001b[K\rremote: Compressing objects:  29% (30/103)\u001b[K\rremote: Compressing objects:  30% (31/103)\u001b[K\rremote: Compressing objects:  31% (32/103)\u001b[K\rremote: Compressing objects:  32% (33/103)\u001b[K\rremote: Compressing objects:  33% (34/103)\u001b[K\rremote: Compressing objects:  34% (36/103)\u001b[K\rremote: Compressing objects:  35% (37/103)\u001b[K\rremote: Compressing objects:  36% (38/103)\u001b[K\rremote: Compressing objects:  37% (39/103)\u001b[K\rremote: Compressing objects:  38% (40/103)\u001b[K\rremote: Compressing objects:  39% (41/103)\u001b[K\rremote: Compressing objects:  40% (42/103)\u001b[K\rremote: Compressing objects:  41% (43/103)\u001b[K\rremote: Compressing objects:  42% (44/103)\u001b[K\rremote: Compressing objects:  43% (45/103)\u001b[K\rremote: Compressing objects:  44% (46/103)\u001b[K\rremote: Compressing objects:  45% (47/103)\u001b[K\rremote: Compressing objects:  46% (48/103)\u001b[K\rremote: Compressing objects:  47% (49/103)\u001b[K\rremote: Compressing objects:  48% (50/103)\u001b[K\rremote: Compressing objects:  49% (51/103)\u001b[K\rremote: Compressing objects:  50% (52/103)\u001b[K\rremote: Compressing objects:  51% (53/103)\u001b[K\rremote: Compressing objects:  52% (54/103)\u001b[K\rremote: Compressing objects:  53% (55/103)\u001b[K\rremote: Compressing objects:  54% (56/103)\u001b[K\rremote: Compressing objects:  55% (57/103)\u001b[K\rremote: Compressing objects:  56% (58/103)\u001b[K\rremote: Compressing objects:  57% (59/103)\u001b[K\rremote: Compressing objects:  58% (60/103)\u001b[K\rremote: Compressing objects:  59% (61/103)\u001b[K\rremote: Compressing objects:  60% (62/103)\u001b[K\rremote: Compressing objects:  61% (63/103)\u001b[K\rremote: Compressing objects:  62% (64/103)\u001b[K\rremote: Compressing objects:  63% (65/103)\u001b[K\rremote: Compressing objects:  64% (66/103)\u001b[K\rremote: Compressing objects:  65% (67/103)\u001b[K\rremote: Compressing objects:  66% (68/103)\u001b[K\rremote: Compressing objects:  67% (70/103)\u001b[K\rremote: Compressing objects:  68% (71/103)\u001b[K\rremote: Compressing objects:  69% (72/103)\u001b[K\rremote: Compressing objects:  70% (73/103)\u001b[K\rremote: Compressing objects:  71% (74/103)\u001b[K\rremote: Compressing objects:  72% (75/103)\u001b[K\rremote: Compressing objects:  73% (76/103)\u001b[K\rremote: Compressing objects:  74% (77/103)\u001b[K\rremote: Compressing objects:  75% (78/103)\u001b[K\rremote: Compressing objects:  76% (79/103)\u001b[K\rremote: Compressing objects:  77% (80/103)\u001b[K\rremote: Compressing objects:  78% (81/103)\u001b[K\rremote: Compressing objects:  79% (82/103)\u001b[K\rremote: Compressing objects:  80% (83/103)\u001b[K\rremote: Compressing objects:  81% (84/103)\u001b[K\rremote: Compressing objects:  82% (85/103)\u001b[K\rremote: Compressing objects:  83% (86/103)\u001b[K\rremote: Compressing objects:  84% (87/103)\u001b[K\rremote: Compressing objects:  85% (88/103)\u001b[K\rremote: Compressing objects:  86% (89/103)\u001b[K\rremote: Compressing objects:  87% (90/103)\u001b[K\rremote: Compressing objects:  88% (91/103)\u001b[K\rremote: Compressing objects:  89% (92/103)\u001b[K\rremote: Compressing objects:  90% (93/103)\u001b[K\rremote: Compressing objects:  91% (94/103)\u001b[K\rremote: Compressing objects:  92% (95/103)\u001b[K\rremote: Compressing objects:  93% (96/103)\u001b[K\rremote: Compressing objects:  94% (97/103)\u001b[K\rremote: Compressing objects:  95% (98/103)\u001b[K\rremote: Compressing objects:  96% (99/103)\u001b[K\rremote: Compressing objects:  97% (100/103)\u001b[K\rremote: Compressing objects:  98% (101/103)\u001b[K\rremote: Compressing objects:  99% (102/103)\u001b[K\rremote: Compressing objects: 100% (103/103)\u001b[K\rremote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "Receiving objects:   0% (1/139)   \rReceiving objects:   1% (2/139)   \rReceiving objects:   2% (3/139)   \rReceiving objects:   3% (5/139)   \rReceiving objects:   4% (6/139)   \rReceiving objects:   5% (7/139)   \rReceiving objects:   6% (9/139)   \rReceiving objects:   7% (10/139)   \rReceiving objects:   8% (12/139)   \rReceiving objects:   9% (13/139)   \rReceiving objects:  10% (14/139)   \rReceiving objects:  11% (16/139)   \rReceiving objects:  12% (17/139)   \rReceiving objects:  13% (19/139)   \rReceiving objects:  14% (20/139)   \rReceiving objects:  15% (21/139)   \rReceiving objects:  16% (23/139)   \rReceiving objects:  17% (24/139)   \rReceiving objects:  18% (26/139)   \rReceiving objects:  19% (27/139)   \rReceiving objects:  20% (28/139)   \rReceiving objects:  21% (30/139)   \rReceiving objects:  22% (31/139)   \rReceiving objects:  23% (32/139)   \rReceiving objects:  24% (34/139)   \rReceiving objects:  25% (35/139)   \rReceiving objects:  26% (37/139)   \rReceiving objects:  27% (38/139)   \rReceiving objects:  28% (39/139)   \rReceiving objects:  29% (41/139)   \rReceiving objects:  30% (42/139)   \rReceiving objects:  31% (44/139)   \rReceiving objects:  32% (45/139)   \rReceiving objects:  33% (46/139)   \rReceiving objects:  34% (48/139)   \rReceiving objects:  35% (49/139)   \rReceiving objects:  36% (51/139)   \rReceiving objects:  37% (52/139)   \rReceiving objects:  38% (53/139)   \rReceiving objects:  39% (55/139)   \rReceiving objects:  40% (56/139)   \rReceiving objects:  41% (57/139)   \rReceiving objects:  42% (59/139)   \rReceiving objects:  43% (60/139)   \rReceiving objects:  44% (62/139)   \rReceiving objects:  45% (63/139)   \rReceiving objects:  46% (64/139)   \rReceiving objects:  47% (66/139)   \rReceiving objects:  48% (67/139)   \rReceiving objects:  49% (69/139)   \rReceiving objects:  50% (70/139)   \rReceiving objects:  51% (71/139)   \rReceiving objects:  52% (73/139)   \rReceiving objects:  53% (74/139)   \rremote: Total 139 (delta 74), reused 86 (delta 34), pack-reused 0\u001b[K\n",
            "Receiving objects:  54% (76/139)   \rReceiving objects:  55% (77/139)   \rReceiving objects:  56% (78/139)   \rReceiving objects:  57% (80/139)   \rReceiving objects:  58% (81/139)   \rReceiving objects:  59% (83/139)   \rReceiving objects:  60% (84/139)   \rReceiving objects:  61% (85/139)   \rReceiving objects:  62% (87/139)   \rReceiving objects:  63% (88/139)   \rReceiving objects:  64% (89/139)   \rReceiving objects:  65% (91/139)   \rReceiving objects:  66% (92/139)   \rReceiving objects:  67% (94/139)   \rReceiving objects:  68% (95/139)   \rReceiving objects:  69% (96/139)   \rReceiving objects:  70% (98/139)   \rReceiving objects:  71% (99/139)   \rReceiving objects:  72% (101/139)   \rReceiving objects:  73% (102/139)   \rReceiving objects:  74% (103/139)   \rReceiving objects:  75% (105/139)   \rReceiving objects:  76% (106/139)   \rReceiving objects:  77% (108/139)   \rReceiving objects:  78% (109/139)   \rReceiving objects:  79% (110/139)   \rReceiving objects:  80% (112/139)   \rReceiving objects:  81% (113/139)   \rReceiving objects:  82% (114/139)   \rReceiving objects:  83% (116/139)   \rReceiving objects:  84% (117/139)   \rReceiving objects:  85% (119/139)   \rReceiving objects:  86% (120/139)   \rReceiving objects:  87% (121/139)   \rReceiving objects:  88% (123/139)   \rReceiving objects:  89% (124/139)   \rReceiving objects:  90% (126/139)   \rReceiving objects:  91% (127/139)   \rReceiving objects:  92% (128/139)   \rReceiving objects:  93% (130/139)   \rReceiving objects:  94% (131/139)   \rReceiving objects:  95% (133/139)   \rReceiving objects:  96% (134/139)   \rReceiving objects:  97% (135/139)   \rReceiving objects:  98% (137/139)   \rReceiving objects:  99% (138/139)   \rReceiving objects: 100% (139/139)   \rReceiving objects: 100% (139/139), 29.23 KiB | 610.00 KiB/s, done.\n",
            "Resolving deltas:   0% (0/74)   \rResolving deltas:  22% (17/74)   \rResolving deltas:  27% (20/74)   \rResolving deltas:  36% (27/74)   \rResolving deltas:  40% (30/74)   \rResolving deltas:  41% (31/74)   \rResolving deltas:  52% (39/74)   \rResolving deltas:  83% (62/74)   \rResolving deltas:  98% (73/74)   \rResolving deltas: 100% (74/74)   \rResolving deltas: 100% (74/74), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd72fnkehjER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec7bcb5e-2be5-44aa-f445-70029536e358"
      },
      "source": [
        "%cd AugMix_TF2/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/AugMix_TF2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW5WOKY9hlbm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9fa504ca-d846-4608-f574-79f508ec0aca"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "augmentation.py  data_generator.py  main.py  README.md\t       trainer.py\n",
            "config.py\t LICENSE\t    models   requirements.txt  utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrJE9DLnaSvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBuIi7c6hmSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a0d5bb1-04cd-4fb8-dc47-49221a4282d5"
      },
      "source": [
        "!python main.py --batch_size=32 --epochs=100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading model\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 32)   4640        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 32)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 32)   9248        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 32)   544         activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 32)   0           conv2d_9[0][0]                   \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 32)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 32)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 32)   0           activation_8[0][0]               \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 32)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 32)   0           activation_10[0][0]              \n",
            "                                                                 batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 64)     18496       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 64)     36928       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 64)     2112        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 64)     0           conv2d_16[0][0]                  \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 64)     0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 64)     0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 64)     0           activation_14[0][0]              \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 64)     0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 64)     0           activation_16[0][0]              \n",
            "                                                                 batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200701_204123-wgit7mph\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33methereal-flower-76\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 猸锔 View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/authors/tfaugmentation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/authors/tfaugmentation/runs/wgit7mph\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "\n",
            "Called with args:\n",
            "batch_size:  32\n",
            "epochs    :  100\n",
            "max_lr    :  1.0\n",
            "min_lr    :  1e-05\n",
            "img_size  :  32\n",
            "save_dir_path:  \n",
            "plot_name :  history.png\n",
            "jsd_loss  :  True\n",
            "severity  :  3\n",
            "width     :  3\n",
            "depth     :  -1\n",
            "alpha     :  1.0\n",
            "==============================================================================\n",
            "\n",
            "Loading data now. Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "Data loading complete. \n",
            "\n",
            "Initializing from scratch.\n",
            "1563/1563 [==============================] - 224s 143ms/step\n",
            "Epoch: 1 \n",
            "                train_loss: 1.624801  train_acc: 48.61%  \n",
            "                test_loss:  1.231311  test_acc:  55.28%\n",
            "\n",
            "val_loss improved from inf to 1.2313. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 206s 132ms/step\n",
            "Epoch: 2 \n",
            "                train_loss: 1.218795  train_acc: 65.22%  \n",
            "                test_loss:  0.920918  test_acc:  68.64%\n",
            "\n",
            "val_loss improved from 1.2313 to 0.9209. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 207s 133ms/step\n",
            "Epoch: 3 \n",
            "                train_loss: 1.033147  train_acc: 72.51%  \n",
            "                test_loss:  0.870951  test_acc:  69.66%\n",
            "\n",
            "val_loss improved from 0.9209 to 0.8710. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 208s 133ms/step\n",
            "Epoch: 4 \n",
            "                train_loss: 0.916953  train_acc: 76.60%  \n",
            "                test_loss:  0.848593  test_acc:  71.03%\n",
            "\n",
            "val_loss improved from 0.8710 to 0.8486. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 205s 131ms/step\n",
            "Epoch: 5 \n",
            "                train_loss: 0.837818  train_acc: 79.67%  \n",
            "                test_loss:  0.731391  test_acc:  74.13%\n",
            "\n",
            "val_loss improved from 0.8486 to 0.7314. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 207s 132ms/step\n",
            "Epoch: 6 \n",
            "                train_loss: 0.784974  train_acc: 81.43%  \n",
            "                test_loss:  0.678986  test_acc:  76.64%\n",
            "\n",
            "val_loss improved from 0.7314 to 0.6790. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 205s 131ms/step\n",
            "Epoch: 7 \n",
            "                train_loss: 0.733303  train_acc: 83.20%  \n",
            "                test_loss:  0.668706  test_acc:  76.66%\n",
            "\n",
            "val_loss improved from 0.6790 to 0.6687. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 205s 131ms/step\n",
            "Epoch: 8 \n",
            "                train_loss: 0.698095  train_acc: 84.69%  \n",
            "                test_loss:  0.608274  test_acc:  79.02%\n",
            "\n",
            "val_loss improved from 0.6687 to 0.6083. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 205s 131ms/step\n",
            "Epoch: 9 \n",
            "                train_loss: 0.659935  train_acc: 85.76%  \n",
            "                test_loss:  0.662287  test_acc:  77.32%\n",
            "\n",
            "val_loss didn't improve\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 206s 132ms/step\n",
            "Epoch: 10 \n",
            "                train_loss: 0.633292  train_acc: 86.88%  \n",
            "                test_loss:  0.631572  test_acc:  78.28%\n",
            "\n",
            "val_loss didn't improve\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 206s 132ms/step\n",
            "Epoch: 11 \n",
            "                train_loss: 0.606582  train_acc: 87.66%  \n",
            "                test_loss:  0.586170  test_acc:  79.25%\n",
            "\n",
            "val_loss improved from 0.6083 to 0.5862. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 207s 132ms/step\n",
            "Epoch: 12 \n",
            "                train_loss: 0.587181  train_acc: 88.50%  \n",
            "                test_loss:  0.558724  test_acc:  80.60%\n",
            "\n",
            "val_loss improved from 0.5862 to 0.5587. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 206s 132ms/step\n",
            "Epoch: 13 \n",
            "                train_loss: 0.570818  train_acc: 88.92%  \n",
            "                test_loss:  0.537019  test_acc:  81.19%\n",
            "\n",
            "val_loss improved from 0.5587 to 0.5370. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 204s 131ms/step\n",
            "Epoch: 14 \n",
            "                train_loss: 0.545714  train_acc: 89.73%  \n",
            "                test_loss:  0.516655  test_acc:  82.11%\n",
            "\n",
            "val_loss improved from 0.5370 to 0.5167. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 203s 130ms/step\n",
            "Epoch: 15 \n",
            "                train_loss: 0.532813  train_acc: 90.26%  \n",
            "                test_loss:  0.512397  test_acc:  82.06%\n",
            "\n",
            "val_loss improved from 0.5167 to 0.5124. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 203s 130ms/step\n",
            "Epoch: 16 \n",
            "                train_loss: 0.512869  train_acc: 91.00%  \n",
            "                test_loss:  0.491795  test_acc:  82.93%\n",
            "\n",
            "val_loss improved from 0.5124 to 0.4918. Saving model checkpoint.\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 201s 128ms/step\n",
            "Epoch: 17 \n",
            "                train_loss: 0.505362  train_acc: 91.42%  \n",
            "                test_loss:  0.562315  test_acc:  80.55%\n",
            "\n",
            "val_loss didn't improve\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            "1563/1563 [==============================] - 200s 128ms/step\n",
            "Epoch: 18 \n",
            "                train_loss: 0.495672  train_acc: 91.80%  \n",
            "                test_loss:  0.568539  test_acc:  79.98%\n",
            "\n",
            "val_loss didn't improve\n",
            "\n",
            "******************************************************************************\n",
            "\n",
            " 160/1563 [==>...........................] - ETA: 2:55"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLb-PCUsRIev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}